{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ff587ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import ccf\n",
    "\n",
    "# Import Feature Engineering/ML Libraries\n",
    "from datetime import date, datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, mean_absolute_percentage_error, r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d60e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping Track of Input Variables\n",
    "path = '/Users/angelobenedicto/Documents/Commodities Hedger/Commodities'\n",
    "target = 'CHK'\n",
    "today = date.today()\n",
    "hedge_period = 12\n",
    "tts = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37e8a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get List of Commodities\n",
    "def list_csv_files(folder_path):\n",
    "    csv_files = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.csv'):\n",
    "            csv_files.append(file)\n",
    "    return csv_files\n",
    "\n",
    "comms = list_csv_files(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "862a2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Total Comm DF\n",
    "total_comm_df = pd.read_csv(f'/Users/angelobenedicto/Documents/Commodities Hedger/Commodities/{comms[0]}')\n",
    "\n",
    "for item in range(len(comms)):\n",
    "    if item == 0:\n",
    "        pass\n",
    "    else:\n",
    "        comm_df = pd.read_csv(f'/Users/angelobenedicto/Documents/Commodities Hedger/Commodities/{comms[item]}')\n",
    "        total_comm_df = total_comm_df.merge(comm_df, how='outer', on='DATE')\n",
    "\n",
    "#Create Date Time For Time esries Analysis\n",
    "total_comm_df['DATE'] = pd.to_datetime(total_comm_df['DATE'])\n",
    "\n",
    "#Fetch Manually Maintained Code Lookup\n",
    "comm_lookup = pd.read_excel('/Users/angelobenedicto/Documents/Commodities Hedger/Commodities/CommLookUp.xlsx')\n",
    "\n",
    "#Get List of Columns\n",
    "comms = list(total_comm_df.columns)\n",
    "comms.remove('DATE')\n",
    "\n",
    "#Get FRED Short Code\n",
    "short_codes = pd.DataFrame(comms).merge(comm_lookup[['FRED Ticker', 'Code']], how='left', left_on=0, right_on='FRED Ticker')\n",
    "short_codes = list(short_codes['Code'])\n",
    "\n",
    "#Add 'DATE' to column names\n",
    "short_codes.insert(0, 'DATE')\n",
    "\n",
    "#Rename Columns to Short Codes\n",
    "total_comm_df.columns=short_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c712a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the complete monthly date range from the earliest to the latest date in total_comm_df\n",
    "date_range = pd.date_range(start=total_comm_df['DATE'].min(), end=total_comm_df['DATE'].max(), freq='MS')\n",
    "\n",
    "# Set the 'DATE' column as the index for reindexing\n",
    "total_comm_df.set_index('DATE', inplace=True)\n",
    "\n",
    "# Reindex the dataframe with the complete monthly date range, filling missing values as NaN\n",
    "continuous_total_comm_df = total_comm_df.reindex(date_range)\n",
    "\n",
    "# If you want to have 'DATE' as a column instead of the index\n",
    "continuous_total_comm_df.reset_index(inplace=True)\n",
    "continuous_total_comm_df.rename(columns={'index': 'DATE'}, inplace=True)\n",
    "\n",
    "#Turn DF into Time Series\n",
    "continuous_total_comm_df.set_index('DATE', inplace=True)\n",
    "\n",
    "#Filter Out Dates Without Target\n",
    "start_date = continuous_total_comm_df[[target]].dropna().index.min()\n",
    "end_date = continuous_total_comm_df[[target]].dropna().index.max()\n",
    "continuous_total_comm_df = continuous_total_comm_df[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "870aac61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Dates:\n",
      "DatetimeIndex([], dtype='datetime64[ns]', freq='MS')\n"
     ]
    }
   ],
   "source": [
    "#Quick Check For Missing Dates\n",
    "\n",
    "# Assuming continuous_total_comm_df is your DataFrame\n",
    "start_date = continuous_total_comm_df.index.min()\n",
    "end_date = continuous_total_comm_df.index.max()\n",
    "\n",
    "# Generate a complete range of monthly dates from start to end\n",
    "complete_date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "# Find missing dates by checking which dates in the complete range are not in the DataFrame's index\n",
    "missing_dates = complete_date_range.difference(continuous_total_comm_df.index)\n",
    "\n",
    "print(\"Missing Dates:\")\n",
    "print(missing_dates)\n",
    "\n",
    "#Drop all NaN Values\n",
    "continuous_total_comm_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36da13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns that should be numeric\n",
    "for col in continuous_total_comm_df.columns:  # List other columns if needed\n",
    "    continuous_total_comm_df[col] = pd.to_numeric(continuous_total_comm_df[col], errors='coerce')\n",
    "\n",
    "# Function to impute values based on the described logic\n",
    "def impute_values(df, column):\n",
    "    for i, value in enumerate(df[column]):\n",
    "        if pd.isnull(value):  # Check if value is NaN (formerly \".\")\n",
    "            prev_val = df[column].iloc[:i].dropna().tail(1).values  # Value before the NaN\n",
    "            next_val = df[column].iloc[i+1:].dropna().head(1).values  # Value after the NaN\n",
    "\n",
    "            if prev_val.size > 0 and next_val.size > 0:\n",
    "                # Average of the period before and the period after\n",
    "                df.at[df.index[i], column] = np.mean([prev_val[0], next_val[0]])\n",
    "            else:\n",
    "                # Impute with the mean of that year\n",
    "                year = df.index[i].year\n",
    "                yearly_mean = df[column][df.index.year == year].mean()\n",
    "                df.at[df.index[i], column] = yearly_mean\n",
    "\n",
    "# Apply the imputation function to each column that requires it\n",
    "for col in continuous_total_comm_df.columns:\n",
    "    if continuous_total_comm_df[col].dtype == 'float64':  # Assuming you want to apply this to float columns\n",
    "        impute_values(continuous_total_comm_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aed82638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Rolling Values\n",
    "for comm in continuous_total_comm_df.columns:\n",
    "    for period in [3, 6, 9, 12]:\n",
    "        continuous_total_comm_df[f'{comm} Rolling {period}M'] = continuous_total_comm_df[comm].rolling(window=period).mean()\n",
    "        \n",
    "#Get Rolling Growth Values\n",
    "for comm in continuous_total_comm_df.columns:\n",
    "    for period in [3, 6, 9, 12]:\n",
    "        continuous_total_comm_df[f'{comm} M-{period}'] = continuous_total_comm_df[comm].shift(+period)\n",
    "        continuous_total_comm_df[f'{comm} M+{period} Growth'] = continuous_total_comm_df[comm] / continuous_total_comm_df[f'{comm} M-{period}'] - 1\n",
    "\n",
    "#Drop NaN Values\n",
    "continuous_total_comm_df.dropna(inplace=True)\n",
    "\n",
    "#Create 3M Future Value of Target\n",
    "continuous_total_comm_df[f'{target} Future Price'] = continuous_total_comm_df[target].shift(-hedge_period)\n",
    "\n",
    "#Get Price Increases\n",
    "continuous_total_comm_df['Increase'] = np.where(continuous_total_comm_df[f'{target} Future Price'] > continuous_total_comm_df[target], 1, 0)\n",
    "\n",
    "#Create lin_reg_df\n",
    "lin_reg_df = continuous_total_comm_df.drop('Increase', axis=1)\n",
    "\n",
    "#Drop Target Future Price\n",
    "continuous_total_comm_df.drop(f'{target} Future Price', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3362c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notes\n",
    "\n",
    "# 1. Conduct Retracement Analysis -> quick walk through with andrew\n",
    "# 2. Take into account inflation, everything is inclined to grow\n",
    "# 3. Conduct a volume sold analysis - try to see if this has an effect on price\n",
    "# 4. Explore signals, conventional trend analytics -> Can I detect a dead cat bounce, bull flag analysis, \n",
    "# 5. Check Random Forest to see which branches were cut first\n",
    "# 6. Conduct PCA Analysis to determine most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8dd649ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"{model_name} F1 Score: {f1}\")\n",
    "    print(f\"{model_name} Confusion Matrix:\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "    print(f\"True Positives (TP): {tp}\\n\")\n",
    "    return f1\n",
    "\n",
    "def run_multi_model(val_date, run_gradient_boost):\n",
    "    val_date_ts = datetime.strptime(val_date, '%Y-%m')\n",
    "    val_date_plus_one_month = val_date_ts + relativedelta(months=+1)\n",
    "    val_date_next = val_date_plus_one_month.strftime('%Y-%m')\n",
    "\n",
    "    df = continuous_total_comm_df[:val_date]\n",
    "    X = df.drop('Increase', axis=1)\n",
    "    y = df['Increase']\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=tts, random_state=42)\n",
    "\n",
    "    f1_scores = []\n",
    "    \n",
    "    print('Test Set Results')\n",
    "    print('')\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    y_pred_lr = lr_model.predict(X_test)\n",
    "    f1_lr = evaluate_model(\"Logistic Regression\", y_test, y_pred_lr)\n",
    "    f1_scores.append([\"Logistic Regression\", f1_lr])\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    f1_xgb = evaluate_model(\"XGBoost\", y_test, y_pred_xgb)\n",
    "    f1_scores.append([\"XGBoost\", f1_xgb])\n",
    "\n",
    "    if run_gradient_boost:\n",
    "        # Gradient Boosting with Grid Search\n",
    "        parameters = {'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7], 'n_estimators': [100, 200]}\n",
    "        gb_model = GridSearchCV(estimator=GradientBoostingClassifier(), param_grid=parameters, scoring='f1', cv=5)\n",
    "        gb_model.fit(X_train, y_train)\n",
    "        best_gb_model = gb_model.best_estimator_\n",
    "        y_pred_gb = best_gb_model.predict(X_test)\n",
    "        f1_gb = evaluate_model(\"Gradient Boosting (Best)\", y_test, y_pred_gb)\n",
    "        f1_scores.append([\"Gradient Boosting (Best)\", f1_gb])\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier()\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    f1_rf = evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
    "    f1_scores.append([\"Random Forest\", f1_rf])\n",
    "\n",
    "    top_model, top_f1 = max(f1_scores, key=lambda x: x[1])\n",
    "\n",
    "    forecast_model = {\n",
    "        \"Logistic Regression\": lr_model,\n",
    "        \"XGBoost\": xgb_model,\n",
    "        \"Random Forest\": rf_model\n",
    "    }[top_model]\n",
    "\n",
    "    val_df = continuous_total_comm_df[val_date_next:'2023-02']\n",
    "    X_val = val_df.drop('Increase', axis=1)\n",
    "    y_val = val_df['Increase']\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    y_val_preds = forecast_model.predict(X_val_scaled)\n",
    "    \n",
    "    print('Hold Out Set Results:')\n",
    "    print('')\n",
    "\n",
    "    f1_val = evaluate_model(top_model, y_val, y_val_preds)\n",
    "\n",
    "    return y_val_preds, y_val, val_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8996d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Results\n",
      "\n",
      "Logistic Regression F1 Score: 0.912621359223301\n",
      "Logistic Regression Confusion Matrix:\n",
      "True Negatives (TN): 12\n",
      "False Positives (FP): 6\n",
      "False Negatives (FN): 3\n",
      "True Positives (TP): 47\n",
      "\n",
      "XGBoost F1 Score: 0.9019607843137256\n",
      "XGBoost Confusion Matrix:\n",
      "True Negatives (TN): 12\n",
      "False Positives (FP): 6\n",
      "False Negatives (FN): 4\n",
      "True Positives (TP): 46\n",
      "\n",
      "Random Forest F1 Score: 0.8686868686868686\n",
      "Random Forest Confusion Matrix:\n",
      "True Negatives (TN): 12\n",
      "False Positives (FP): 6\n",
      "False Negatives (FN): 7\n",
      "True Positives (TP): 43\n",
      "\n",
      "Hold Out Set Results:\n",
      "\n",
      "Logistic Regression F1 Score: 0.8571428571428571\n",
      "Logistic Regression Confusion Matrix:\n",
      "True Negatives (TN): 0\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 3\n",
      "True Positives (TP): 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run Logistic Regression\n",
    "y_val_preds, y_val, val_df = run_multi_model(val_date='2022-02', run_gradient_boost=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c82a59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag 0</th>\n",
       "      <th>Lag 1</th>\n",
       "      <th>Lag 2</th>\n",
       "      <th>Lag 3</th>\n",
       "      <th>Lag 4</th>\n",
       "      <th>Lag 5</th>\n",
       "      <th>Lag 6</th>\n",
       "      <th>Lag 7</th>\n",
       "      <th>Lag 8</th>\n",
       "      <th>Lag 9</th>\n",
       "      <th>Lag 10</th>\n",
       "      <th>Lag 11</th>\n",
       "      <th>Average of Lags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHK</th>\n",
       "      <td>0.947848</td>\n",
       "      <td>0.940008</td>\n",
       "      <td>0.930471</td>\n",
       "      <td>0.920403</td>\n",
       "      <td>0.910646</td>\n",
       "      <td>0.902902</td>\n",
       "      <td>0.895114</td>\n",
       "      <td>0.885583</td>\n",
       "      <td>0.878596</td>\n",
       "      <td>0.870978</td>\n",
       "      <td>0.864201</td>\n",
       "      <td>0.859102</td>\n",
       "      <td>0.900488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEF</th>\n",
       "      <td>0.931393</td>\n",
       "      <td>0.924204</td>\n",
       "      <td>0.916113</td>\n",
       "      <td>0.908179</td>\n",
       "      <td>0.899368</td>\n",
       "      <td>0.891441</td>\n",
       "      <td>0.883095</td>\n",
       "      <td>0.873192</td>\n",
       "      <td>0.863733</td>\n",
       "      <td>0.853217</td>\n",
       "      <td>0.843804</td>\n",
       "      <td>0.835375</td>\n",
       "      <td>0.88526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY</th>\n",
       "      <td>0.953788</td>\n",
       "      <td>0.941732</td>\n",
       "      <td>0.92833</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.902253</td>\n",
       "      <td>0.88995</td>\n",
       "      <td>0.878192</td>\n",
       "      <td>0.864776</td>\n",
       "      <td>0.852855</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.825724</td>\n",
       "      <td>0.813811</td>\n",
       "      <td>0.88384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELE</th>\n",
       "      <td>0.919554</td>\n",
       "      <td>0.909917</td>\n",
       "      <td>0.899249</td>\n",
       "      <td>0.887888</td>\n",
       "      <td>0.875449</td>\n",
       "      <td>0.864044</td>\n",
       "      <td>0.853431</td>\n",
       "      <td>0.842534</td>\n",
       "      <td>0.834131</td>\n",
       "      <td>0.824215</td>\n",
       "      <td>0.814525</td>\n",
       "      <td>0.805196</td>\n",
       "      <td>0.860844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOY</th>\n",
       "      <td>0.822226</td>\n",
       "      <td>0.813923</td>\n",
       "      <td>0.80303</td>\n",
       "      <td>0.792003</td>\n",
       "      <td>0.781991</td>\n",
       "      <td>0.774009</td>\n",
       "      <td>0.76833</td>\n",
       "      <td>0.760572</td>\n",
       "      <td>0.754708</td>\n",
       "      <td>0.744423</td>\n",
       "      <td>0.732428</td>\n",
       "      <td>0.721671</td>\n",
       "      <td>0.772443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRN</th>\n",
       "      <td>0.737674</td>\n",
       "      <td>0.727714</td>\n",
       "      <td>0.714709</td>\n",
       "      <td>0.69977</td>\n",
       "      <td>0.683702</td>\n",
       "      <td>0.666912</td>\n",
       "      <td>0.651528</td>\n",
       "      <td>0.635082</td>\n",
       "      <td>0.620582</td>\n",
       "      <td>0.603216</td>\n",
       "      <td>0.585225</td>\n",
       "      <td>0.570377</td>\n",
       "      <td>0.658041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHT</th>\n",
       "      <td>0.721158</td>\n",
       "      <td>0.712795</td>\n",
       "      <td>0.701497</td>\n",
       "      <td>0.68862</td>\n",
       "      <td>0.674513</td>\n",
       "      <td>0.659996</td>\n",
       "      <td>0.648077</td>\n",
       "      <td>0.633874</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.603339</td>\n",
       "      <td>0.584933</td>\n",
       "      <td>0.570184</td>\n",
       "      <td>0.651582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WTI</th>\n",
       "      <td>0.649</td>\n",
       "      <td>0.638402</td>\n",
       "      <td>0.626165</td>\n",
       "      <td>0.614702</td>\n",
       "      <td>0.602254</td>\n",
       "      <td>0.588811</td>\n",
       "      <td>0.576389</td>\n",
       "      <td>0.561836</td>\n",
       "      <td>0.545416</td>\n",
       "      <td>0.527785</td>\n",
       "      <td>0.512456</td>\n",
       "      <td>0.498474</td>\n",
       "      <td>0.578474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POT</th>\n",
       "      <td>0.499968</td>\n",
       "      <td>0.504534</td>\n",
       "      <td>0.505968</td>\n",
       "      <td>0.506373</td>\n",
       "      <td>0.500917</td>\n",
       "      <td>0.499073</td>\n",
       "      <td>0.498426</td>\n",
       "      <td>0.495864</td>\n",
       "      <td>0.492607</td>\n",
       "      <td>0.48529</td>\n",
       "      <td>0.476146</td>\n",
       "      <td>0.462589</td>\n",
       "      <td>0.49398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAS</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.002675</td>\n",
       "      <td>-0.006403</td>\n",
       "      <td>-0.010359</td>\n",
       "      <td>-0.015595</td>\n",
       "      <td>-0.022942</td>\n",
       "      <td>-0.030128</td>\n",
       "      <td>-0.040478</td>\n",
       "      <td>-0.050085</td>\n",
       "      <td>-0.057283</td>\n",
       "      <td>-0.065277</td>\n",
       "      <td>-0.074768</td>\n",
       "      <td>-0.031333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lag 0     Lag 1     Lag 2     Lag 3     Lag 4     Lag 5     Lag 6  \\\n",
       "CHK  0.947848  0.940008  0.930471  0.920403  0.910646  0.902902  0.895114   \n",
       "BEF  0.931393  0.924204  0.916113  0.908179  0.899368  0.891441  0.883095   \n",
       "PAY  0.953788  0.941732   0.92833  0.915663  0.902253   0.88995  0.878192   \n",
       "ELE  0.919554  0.909917  0.899249  0.887888  0.875449  0.864044  0.853431   \n",
       "SOY  0.822226  0.813923   0.80303  0.792003  0.781991  0.774009   0.76833   \n",
       "CRN  0.737674  0.727714  0.714709   0.69977  0.683702  0.666912  0.651528   \n",
       "WHT  0.721158  0.712795  0.701497   0.68862  0.674513  0.659996  0.648077   \n",
       "WTI     0.649  0.638402  0.626165  0.614702  0.602254  0.588811  0.576389   \n",
       "POT  0.499968  0.504534  0.505968  0.506373  0.500917  0.499073  0.498426   \n",
       "GAS -0.000001 -0.002675 -0.006403 -0.010359 -0.015595 -0.022942 -0.030128   \n",
       "\n",
       "        Lag 7     Lag 8     Lag 9    Lag 10    Lag 11 Average of Lags  \n",
       "CHK  0.885583  0.878596  0.870978  0.864201  0.859102        0.900488  \n",
       "BEF  0.873192  0.863733  0.853217  0.843804  0.835375         0.88526  \n",
       "PAY  0.864776  0.852855     0.839  0.825724  0.813811         0.88384  \n",
       "ELE  0.842534  0.834131  0.824215  0.814525  0.805196        0.860844  \n",
       "SOY  0.760572  0.754708  0.744423  0.732428  0.721671        0.772443  \n",
       "CRN  0.635082  0.620582  0.603216  0.585225  0.570377        0.658041  \n",
       "WHT  0.633874      0.62  0.603339  0.584933  0.570184        0.651582  \n",
       "WTI  0.561836  0.545416  0.527785  0.512456  0.498474        0.578474  \n",
       "POT  0.495864  0.492607   0.48529  0.476146  0.462589         0.49398  \n",
       "GAS -0.040478 -0.050085 -0.057283 -0.065277 -0.074768       -0.031333  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = lin_reg_df  # Assuming the first column is a datetime index\n",
    "\n",
    "# List of features to analyze\n",
    "features = ['SOY', 'CHK', 'POT', 'WTI', 'GAS', 'PAY', 'ELE', 'WHT', 'BEF', 'CRN']\n",
    "target = 'CHK Future Price'  # Assuming this is your target variable\n",
    "\n",
    "# Maximum number of lags to check\n",
    "lag_max = 12\n",
    "\n",
    "# Prepare a DataFrame to store CCF results\n",
    "ccf_results = pd.DataFrame(index=features, columns=[f'Lag {i}' for i in range(lag_max)] + ['Average of Lags'])\n",
    "\n",
    "# Ensure the data is not missing and fill or interpolate if necessary\n",
    "data = data[features + [target]].dropna()\n",
    "\n",
    "for feature in features:\n",
    "    # Compute Cross-Correlation Function (CCF)\n",
    "    ccf_values = ccf(data[feature], data[target], adjusted=False)[:lag_max]\n",
    "    \n",
    "    # Assign the results to the DataFrame using .iloc for correct slicing\n",
    "    ccf_results.loc[feature, :f'Lag {lag_max-1}'] = ccf_values\n",
    "    \n",
    "    # Compute the average of the lags and assign to the 'Average of Lags' column\n",
    "    ccf_results.loc[feature, 'Average of Lags'] = np.mean(ccf_values)\n",
    "\n",
    "# Display the results table\n",
    "ccf_results.sort_values(by='Average of Lags', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e36ae85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_results.to_excel('ccf_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750be4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
